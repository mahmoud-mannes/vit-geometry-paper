@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{
chu2023conditional,
title={Conditional Positional Encodings for Vision Transformers},
author={Xiangxiang Chu and Zhi Tian and Bo Zhang and Xinlong Wang and Chunhua Shen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3KWnuT-R1bh}
}

@inproceedings{
raghu2021do,
title={Do Vision Transformers See Like Convolutional Neural Networks?},
author={Maithra Raghu and Thomas Unterthiner and Simon Kornblith and Chiyuan Zhang and Alexey Dosovitskiy},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=R-616EWWKF5}
}

@inproceedings{
Islam*2020How,
title={How much Position Information Do Convolutional Neural Networks Encode?},
author={Md Amirul Islam* and Sen Jia* and Neil D. B. Bruce},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJeB36NKvB}
}
@inproceedings{
kazemnejad2023the,
title={The Impact of Positional Encoding on Length Generalization in Transformers},
author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan and Payel Das and Siva Reddy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Drrl2gcjzl}
}

@INPROCEEDINGS {Liu2021swin,
author = { Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining },
booktitle = { 2021 IEEE/CVF International Conference on Computer Vision (ICCV) },
title = {{ Swin Transformer: Hierarchical Vision Transformer using Shifted Windows }},
year = {2021},
pages = {9992-10002},
doi = {10.1109/ICCV48922.2021.00986},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00986},
publisher = {IEEE Computer Society},
month =Oct}

@article{d’Ascoli_2022,
doi = {10.1088/1742-5468/ac9830},
url = {https://doi.org/10.1088/1742-5468/ac9830},
year = {2022},
month = {nov},
publisher = {IOP Publishing and SISSA},
volume = {2022},
number = {11},
pages = {114005},
author = {d’Ascoli, Stéphane and Touvron, Hugo and Leavitt, Matthew L and Morcos, Ari S and Biroli, Giulio and Sagun, Levent},
title = {ConViT: improving vision transformers with soft convolutional inductive biases*},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
}

@inproceedings{kobayashi-etal-2021-incorporating,
    title = "{I}ncorporating {R}esidual and {N}ormalization {L}ayers into {A}nalysis of {M}asked {L}anguage {M}odels",
    author = "Kobayashi, Goro  and
      Kuribayashi, Tatsuki  and
      Yokoi, Sho  and
      Inui, Kentaro",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.373/",
    doi = "10.18653/v1/2021.emnlp-main.373",
    pages = "4547--4568",
}

@InProceedings{pmlr-v139-dong21a,
  title = 	 {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
  author =       {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2793--2803},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dong21a/dong21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dong21a.html},
}

@inproceedings{Bhojanapalli2021robust,
author = {Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
year = {2021},
month = {10},
pages = {10211-10221},
title = {Understanding Robustness of Transformers for Image Classification},
doi = {10.1109/ICCV48922.2021.01007}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{Paul_Chen_2022, title={Vision Transformers Are Robust Learners}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/20103}, DOI={10.1609/aaai.v36i2.20103}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Paul, Sayak and Chen, Pin-Yu}, year={2022}, month={Jun.}, pages={2071-2081} }

@inproceedings{
geirhos2018imagenettrained,
title={ImageNet-trained {CNN}s are biased towards texture; increasing shape bias improves accuracy and robustness.},
author={Robert Geirhos and Patricia Rubisch and Claudio Michaelis and Matthias Bethge and Felix A. Wichmann and Wieland Brendel},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bygh9j09KX},
}