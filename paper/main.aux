\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dosovitskiy2021an}
\citation{chu2023conditional}
\citation{Islam*2020How}
\citation{raghu2021do}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Setup}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Vision Transformer Architecture}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Positional Embedding Ablation}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Datasets}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Residual Stream Geometry}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spatial Similarity Distance Correlation}{2}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fragility Score}{2}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {SSDC across depth for an untrained ablated model.} SSDC remains approximately constant and at a relatively high value across layers, indicating static spatial correlations induced by architectural and data priors rather than learning. Shaded regions indicate variability across runs (±1 standard deviation).}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SSDC}{{1}{3}{\textbf {SSDC across depth for an untrained ablated model.} SSDC remains approximately constant and at a relatively high value across layers, indicating static spatial correlations induced by architectural and data priors rather than learning. Shaded regions indicate variability across runs (±1 standard deviation)}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architectural Priors Induce Static Spatial Correlations at Initialization}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Validating Emergent Spatial Structure via Extreme Counterfactuals}{3}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {SSDC across depth for an untrained ablated model with random permutation, a trained ablated model, and a trained intact model.} Shaded regions indicate variability across runs (±1 standard deviation).}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:SSDC42}{{2}{3}{\textbf {SSDC across depth for an untrained ablated model with random permutation, a trained ablated model, and a trained intact model.} Shaded regions indicate variability across runs (±1 standard deviation)}{figure.caption.2}{}}
\newlabel{fig:cosine-ablated-untrained-RPI-0}{{3a}{3}{Untrained Ablated Model with Random Permutation Layer 0}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-untrained-RPI-0}{{a}{3}{Untrained Ablated Model with Random Permutation Layer 0}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-untrained-RPI-2}{{3b}{3}{Untrained Ablated Model with Random Permutation Layer 2}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-untrained-RPI-2}{{b}{3}{Untrained Ablated Model with Random Permutation Layer 2}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-0}{{3c}{3}{Trained Ablated Model Layer 0}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-0}{{c}{3}{Trained Ablated Model Layer 0}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-2}{{3d}{3}{Trained Ablated Model Layer 2}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-2}{{d}{3}{Trained Ablated Model Layer 2}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \textbf  {Representative token cosine similarity matrices.} All matrices share the same color scale. }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:token-cosine-similarity-matrices-2}{{3}{3}{\textbf {Representative token cosine similarity matrices.} All matrices share the same color scale}{figure.caption.3}{}}
\newlabel{fig:er_values}{{4a}{4}{Effective Rank across depth (Intact vs Post-hoc Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{sub@fig:er_values}{{a}{4}{Effective Rank across depth (Intact vs Post-hoc Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{fig:cos_sim_values}{{4b}{4}{Mean Token Cosine Similarity across depth (Intact vs Post-hoc Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{sub@fig:cos_sim_values}{{b}{4}{Mean Token Cosine Similarity across depth (Intact vs Post-hoc Ablated vs Untrained Intact)}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Early-layer representational diversity under intact and post-hoc ablated positional embeddings. Left: Effective Rank of the residual stream across layers. Right: Mean token-wise cosine similarity across layers. Results are shown for a fully intact ViT, the same trained model with positional embeddings removed post-hoc at inference time, and a completely untrained intact model. The intact model exhibits substantially higher Effective Rank at layer 0 followed by an early collapse, while the post-hoc ablated model starts from a low-rank regime and remains consistently lower. Mean token cosine similarity is higher for the post-hoc ablated model across all layers. }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:4_3_main}{{4}{4}{Early-layer representational diversity under intact and post-hoc ablated positional embeddings. Left: Effective Rank of the residual stream across layers. Right: Mean token-wise cosine similarity across layers. Results are shown for a fully intact ViT, the same trained model with positional embeddings removed post-hoc at inference time, and a completely untrained intact model. The intact model exhibits substantially higher Effective Rank at layer 0 followed by an early collapse, while the post-hoc ablated model starts from a low-rank regime and remains consistently lower. Mean token cosine similarity is higher for the post-hoc ablated model across all layers}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Positional Embeddings Inject Early-Layer Representational Diversity}{4}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {SSDC across depth for intact models, and intact/ablated models that have had their tokens permuted randomly at inference.} The SSDC collapses to near-zero values upon permutation of the tokens of the ablated model, whereas the intact model's SSDC only takes a slight hit after permutation. Shaded regions indicate variability across runs (±1 standard deviation). }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:SSDC_44}{{5}{5}{\textbf {SSDC across depth for intact models, and intact/ablated models that have had their tokens permuted randomly at inference.} The SSDC collapses to near-zero values upon permutation of the tokens of the ablated model, whereas the intact model's SSDC only takes a slight hit after permutation. Shaded regions indicate variability across runs (±1 standard deviation)}{figure.caption.5}{}}
\newlabel{fig:TCSM_int_44}{{6a}{5}{Token Cosine Similarity Matrix for an intact model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{sub@fig:TCSM_int_44}{{a}{5}{Token Cosine Similarity Matrix for an intact model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{fig:TCSM_abl_44}{{6b}{5}{Token Cosine Similarity Matrix for an ablated model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{sub@fig:TCSM_abl_44}{{b}{5}{Token Cosine Similarity Matrix for an ablated model (permuted) at layer 2}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Representative Token Cosine Similarity Matrices.} Once its tokens are permuted, the ablated model's Token Cosine Similarity Matrix becomes chaotic and loses all spatial structure, whereas the diagonal band persists (though fuzzier) in the intact model's. }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:TCSM_44}{{6}{5}{\textbf {Representative Token Cosine Similarity Matrices.} Once its tokens are permuted, the ablated model's Token Cosine Similarity Matrix becomes chaotic and loses all spatial structure, whereas the diagonal band persists (though fuzzier) in the intact model's}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Patch-Relative and Absolute-Position-Based Modes of Spatial Organization}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Robustness Is Tightly Linked to Spatial Encoding Strategy}{5}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Fragility scores for intact, ablated, and permutation-trained intact models under the stylized dataset.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings.}}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:FS45}{{7}{6}{\textbf {Fragility scores for intact, ablated, and permutation-trained intact models under the stylized dataset.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Fragility scores for intact, ablated, and permutation-trained intact models exposed to Gaussian Blur.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings.}}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig:FS45_GB}{{8}{6}{\textbf {Fragility scores for intact, ablated, and permutation-trained intact models exposed to Gaussian Blur.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\citation{dosovitskiy2021an}
\citation{dosovitskiy2021an,chu2023conditional}
\citation{kazemnejad2023the}
\citation{Islam*2020How}
\citation{d’Ascoli_2022,Liu2021swin}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{7}{section.6}\protected@file@percent }
\citation{raghu2021do}
\citation{kobayashi-etal-2021-incorporating}
\citation{pmlr-v139-dong21a}
\citation{Bhojanapalli2021robust}
\citation{elhage2021mathematical}
\citation{Bhojanapalli2021robust}
\citation{Paul_Chen_2022}
\citation{geirhos2018imagenettrained}
\citation{dosovitskiy2021an,chu2023conditional}
\bibdata{references}
\bibcite{Bhojanapalli2021robust}{{1}{2021}{{Bhojanapalli et~al.}}{{Bhojanapalli, Chakrabarti, Glasner, Li, Unterthiner, and Veit}}}
\bibcite{chu2023conditional}{{2}{2023}{{Chu et~al.}}{{Chu, Tian, Zhang, Wang, and Shen}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\bibcite{pmlr-v139-dong21a}{{3}{2021}{{Dong et~al.}}{{Dong, Cordonnier, and Loukas}}}
\bibcite{dosovitskiy2021an}{{4}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{d’Ascoli_2022}{{5}{2022}{{d’Ascoli et~al.}}{{d’Ascoli, Touvron, Leavitt, Morcos, Biroli, and Sagun}}}
\bibcite{elhage2021mathematical}{{6}{2021}{{Elhage et~al.}}{{Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah}}}
\bibcite{geirhos2018imagenettrained}{{7}{2019}{{Geirhos et~al.}}{{Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and Brendel}}}
\bibcite{Islam*2020How}{{8}{2020}{{Islam* et~al.}}{{Islam*, Jia*, and Bruce}}}
\bibcite{kazemnejad2023the}{{9}{2023}{{Kazemnejad et~al.}}{{Kazemnejad, Padhi, Natesan, Das, and Reddy}}}
\bibcite{kobayashi-etal-2021-incorporating}{{10}{2021}{{Kobayashi et~al.}}{{Kobayashi, Kuribayashi, Yokoi, and Inui}}}
\bibcite{Liu2021swin}{{11}{2021}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{Paul_Chen_2022}{{12}{2022}{{Paul \& Chen}}{{Paul and Chen}}}
\bibcite{raghu2021do}{{13}{2021}{{Raghu et~al.}}{{Raghu, Unterthiner, Kornblith, Zhang, and Dosovitskiy}}}
\bibstyle{icml2026}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental Setup and Hyperparameters}{10}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model architecture and training hyperparameters used in all experiments.}}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:hyperparams}{{1}{10}{Model architecture and training hyperparameters used in all experiments}{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Metric Definitions and Implementation Details}{10}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Spatial Similarity Distance Correlation (SSDC)}{10}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Mean Cosine Similarity}{11}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Fragility Score}{11}{subsection.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Metric Summary}{11}{subsection.B.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces random}}{12}{figure.caption.11}\protected@file@percent }
\newlabel{random}{{9}{12}{random}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional Representational Analyses}{12}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Spatial Structure in Untrained Intact Models}{12}{subsection.C.1}\protected@file@percent }
\gdef \@abspage@last{12}
