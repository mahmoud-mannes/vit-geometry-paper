\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dosovitskiy2021an}
\citation{chu2023conditional}
\citation{Islam*2020How}
\citation{raghu2021do}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Setup}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Vision Transformer Architecture}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Positional Embedding Ablation}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Datasets}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Residual Stream Geometry}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Spatial Similarity Distance Correlation}{2}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Fragility Score}{2}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {SSDC across depth for an untrained ablated model.} SSDC remains approximately constant and at a relatively high value across layers, indicating static spatial correlations induced by architectural and data priors rather than learning. Shaded regions indicate variability across runs (±1 standard deviation).}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SSDC}{{1}{3}{\textbf {SSDC across depth for an untrained ablated model.} SSDC remains approximately constant and at a relatively high value across layers, indicating static spatial correlations induced by architectural and data priors rather than learning. Shaded regions indicate variability across runs (±1 standard deviation)}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architectural Priors Induce Static Spatial Correlations at Initialization}{3}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Validating Emergent Spatial Structure via Extreme Counterfactuals}{3}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {SSDC across depth for an untrained ablated model with random permutation, a trained ablated model, and a trained intact model.}}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:SSDC_42}{{2}{3}{\textbf {SSDC across depth for an untrained ablated model with random permutation, a trained ablated model, and a trained intact model.}}{figure.caption.2}{}}
\newlabel{fig:cosine-ablated-untrained-RPI-0}{{3a}{3}{Untrained Ablated Model with Random Permutation Layer 0}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-untrained-RPI-0}{{a}{3}{Untrained Ablated Model with Random Permutation Layer 0}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-untrained-RPI-2}{{3b}{3}{Untrained Ablated Model with Random Permutation Layer 2}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-untrained-RPI-2}{{b}{3}{Untrained Ablated Model with Random Permutation Layer 2}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-0}{{3c}{3}{Trained Ablated Model Layer 0}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-0}{{c}{3}{Trained Ablated Model Layer 0}{figure.caption.3}{}}
\newlabel{fig:cosine-ablated-2}{{3d}{3}{Trained Ablated Model Layer 2}{figure.caption.3}{}}
\newlabel{sub@fig:cosine-ablated-2}{{d}{3}{Trained Ablated Model Layer 2}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  \textbf  {Representative token cosine similarity matrices.} All matrices share the same color scale. }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:token-cosine-similarity-matrices-2}{{3}{3}{\textbf {Representative token cosine similarity matrices.} All matrices share the same color scale}{figure.caption.3}{}}
\newlabel{fig:er_values}{{4a}{4}{Effective Rank across depth (Intact vs Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{sub@fig:er_values}{{a}{4}{Effective Rank across depth (Intact vs Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{fig:cos_sim_values}{{4b}{4}{Mean Token Cosine Similarity across depth (Intact vs Ablated vs Untrained Intact)}{figure.caption.4}{}}
\newlabel{sub@fig:cos_sim_values}{{b}{4}{Mean Token Cosine Similarity across depth (Intact vs Ablated vs Untrained Intact)}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Early-layer representational diversity under intact and ablated positional embeddings. Left: Effective Rank of the residual stream across layers. Right: Mean token-wise cosine similarity across layers. Results are shown for a fully intact ViT, an ablated model with PEs removed during training and at inference, and a completely untrained intact model. The intact model exhibits substantially higher Effective Rank at layer 0 followed by an early collapse, while the ablated model starts from a low-rank regime and remains lower on average until later layers. Mean token cosine similarity provides a complementary view, as it is higher in the ablated model than in the intact model on average across all layers. }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:4_3_main}{{4}{4}{Early-layer representational diversity under intact and ablated positional embeddings. Left: Effective Rank of the residual stream across layers. Right: Mean token-wise cosine similarity across layers. Results are shown for a fully intact ViT, an ablated model with PEs removed during training and at inference, and a completely untrained intact model. The intact model exhibits substantially higher Effective Rank at layer 0 followed by an early collapse, while the ablated model starts from a low-rank regime and remains lower on average until later layers. Mean token cosine similarity provides a complementary view, as it is higher in the ablated model than in the intact model on average across all layers}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Positional Embeddings Inject Early-Layer Representational Diversity}{4}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {SSDC across depth for intact models, and intact/ablated models that have had their tokens permuted randomly at inference.} The SSDC collapses to near-zero values upon permutation of the tokens of the ablated model, whereas the intact model's SSDC only takes a slight hit after permutation. }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:SSDC_44}{{5}{5}{\textbf {SSDC across depth for intact models, and intact/ablated models that have had their tokens permuted randomly at inference.} The SSDC collapses to near-zero values upon permutation of the tokens of the ablated model, whereas the intact model's SSDC only takes a slight hit after permutation}{figure.caption.5}{}}
\newlabel{fig:TCSM_int_44}{{6a}{5}{Token Cosine Similarity Matrix for an intact model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{sub@fig:TCSM_int_44}{{a}{5}{Token Cosine Similarity Matrix for an intact model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{fig:TCSM_abl_44}{{6b}{5}{Token Cosine Similarity Matrix for an ablated model (permuted) at layer 2}{figure.caption.6}{}}
\newlabel{sub@fig:TCSM_abl_44}{{b}{5}{Token Cosine Similarity Matrix for an ablated model (permuted) at layer 2}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Representative Token Cosine Similarity Matrices.} Once its tokens are permuted, the ablated model's Token Cosine Similarity Matrix becomes chaotic and loses all spatial structure, whereas the diagonal band persists (though fuzzier) in the intact model's. }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:TCSM_44}{{6}{5}{\textbf {Representative Token Cosine Similarity Matrices.} Once its tokens are permuted, the ablated model's Token Cosine Similarity Matrix becomes chaotic and loses all spatial structure, whereas the diagonal band persists (though fuzzier) in the intact model's}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Patch-Relative and Absolute-Position-Based Modes of Spatial Organization}{5}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Robustness Is Tightly Linked to Spatial Encoding Strategy}{5}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Fragility scores for intact, ablated, and permutation-trained intact models under the stylized dataset.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings.}}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:FS45}{{7}{6}{\textbf {Fragility scores for intact, ablated, and permutation-trained intact models under the stylized dataset.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Fragility scores for intact, ablated, and permutation-trained intact models exposed to Gaussian Blur.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings.}}{6}{figure.caption.8}\protected@file@percent }
\newlabel{fig:FS45_GB}{{8}{6}{\textbf {Fragility scores for intact, ablated, and permutation-trained intact models exposed to Gaussian Blur.} Black markers show the mean Fragility Scores with ±1 standard deviation. Intact models are substantially more robust, while permutation-trained intact models exhibit fragility comparable to ablated models despite retaining positional embeddings}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}\protected@file@percent }
\citation{dosovitskiy2021an}
\citation{dosovitskiy2021an,chu2023conditional}
\citation{kazemnejad2023the}
\citation{Islam*2020How}
\citation{d’Ascoli_2022,Liu2021swin}
\citation{dosovitskiy2021an,chu2023conditional}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{7}{section.6}\protected@file@percent }
\citation{raghu2021do}
\citation{kobayashi-etal-2021-incorporating}
\citation{pmlr-v139-dong21a}
\citation{Bhojanapalli2021robust}
\citation{elhage2021mathematical}
\citation{Bhojanapalli2021robust}
\citation{Paul_Chen_2022}
\citation{geirhos2018imagenettrained}
\citation{dosovitskiy2021an,chu2023conditional}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\bibcite{Bhojanapalli2021robust}{{1}{2021}{{Bhojanapalli et~al.}}{{Bhojanapalli, Chakrabarti, Glasner, Li, Unterthiner, and Veit}}}
\bibcite{chu2023conditional}{{2}{2023}{{Chu et~al.}}{{Chu, Tian, Zhang, Wang, and Shen}}}
\bibcite{pmlr-v139-dong21a}{{3}{2021}{{Dong et~al.}}{{Dong, Cordonnier, and Loukas}}}
\bibcite{dosovitskiy2021an}{{4}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{d’Ascoli_2022}{{5}{2022}{{d’Ascoli et~al.}}{{d’Ascoli, Touvron, Leavitt, Morcos, Biroli, and Sagun}}}
\bibcite{elhage2021mathematical}{{6}{2021}{{Elhage et~al.}}{{Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah}}}
\bibcite{geirhos2018imagenettrained}{{7}{2019}{{Geirhos et~al.}}{{Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and Brendel}}}
\bibcite{Islam*2020How}{{8}{2020}{{Islam* et~al.}}{{Islam*, Jia*, and Bruce}}}
\bibcite{kazemnejad2023the}{{9}{2023}{{Kazemnejad et~al.}}{{Kazemnejad, Padhi, Natesan, Das, and Reddy}}}
\bibcite{kobayashi-etal-2021-incorporating}{{10}{2021}{{Kobayashi et~al.}}{{Kobayashi, Kuribayashi, Yokoi, and Inui}}}
\bibcite{Liu2021swin}{{11}{2021}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{Paul_Chen_2022}{{12}{2022}{{Paul \& Chen}}{{Paul and Chen}}}
\bibcite{raghu2021do}{{13}{2021}{{Raghu et~al.}}{{Raghu, Unterthiner, Kornblith, Zhang, and Dosovitskiy}}}
\bibstyle{icml2026}
\@writefile{toc}{\contentsline {section}{\numberline {8}Impact Statements}{9}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental Setup and Hyperparameters}{10}{appendix.A}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model architecture and training hyperparameters used in all experiments.}}{10}{table.caption.10}\protected@file@percent }
\newlabel{tab:hyperparams}{{1}{10}{Model architecture and training hyperparameters used in all experiments}{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Metric Definitions and Implementation Details}{10}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Spatial Similarity Distance Correlation (SSDC)}{10}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Mean Cosine Similarity}{11}{subsection.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Fragility Score}{11}{subsection.B.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Metric Summary}{11}{subsection.B.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional Analyses}{12}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Spatial Structure in Untrained Ablated Models}{12}{subsection.C.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Batch-averaged token cosine similarity heatmaps for an untrained ablated (NoPE) model at layers 0 (top) and 2 (bottom). A diagonal spatial structure is present at initialization, but remains unchanged across layers, indicating static, architecture-induced spatial organization rather than learned positional structure.}}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:APP_C1}{{9}{12}{Batch-averaged token cosine similarity heatmaps for an untrained ablated (NoPE) model at layers 0 (top) and 2 (bottom). A diagonal spatial structure is present at initialization, but remains unchanged across layers, indicating static, architecture-induced spatial organization rather than learned positional structure}{figure.caption.11}{}}
\newlabel{fig:APP_C2_SSDC}{{10a}{13}{\textbf {SSDC Across Depth for an Untrained Intact Model.}SSDC remains consistently low and invariant across depth (n = 3 seeds), indicating an absence of coherent spatial structure despite the presence of positional embeddings}{figure.caption.12}{}}
\newlabel{sub@fig:APP_C2_SSDC}{{a}{13}{\textbf {SSDC Across Depth for an Untrained Intact Model.}SSDC remains consistently low and invariant across depth (n = 3 seeds), indicating an absence of coherent spatial structure despite the presence of positional embeddings}{figure.caption.12}{}}
\newlabel{fig:APP_C2_TCSM}{{10b}{13}{\textbf {Representative Token Cosine Similarity Matrix from an Untrained Intact Model.} The weak diagonal pattern and low overall inter-token similarity reflect disrupted spatial correlations induced by untrained positional embeddings}{figure.caption.12}{}}
\newlabel{sub@fig:APP_C2_TCSM}{{b}{13}{\textbf {Representative Token Cosine Similarity Matrix from an Untrained Intact Model.} The weak diagonal pattern and low overall inter-token similarity reflect disrupted spatial correlations induced by untrained positional embeddings}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces (}}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:APP_C2_main}{{10}{13}{(}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Absence of Spatial Structure in Untrained Intact Models}{13}{subsection.C.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Effects of Post-hoc Ablation of Positional Embeddings}{14}{subsection.C.3}\protected@file@percent }
\newlabel{fig:APP_C3_MCS}{{11a}{14}{\textbf {Mean Cosine Similarity Across Depth With and Without Positional Embeddings} Removing positional embeddings from a trained Intact model increases similarity among token representations in early layers, reflecting a collapse of representational diversity. Intact models maintain lower early-layer similarity, highlighting the role of positional embeddings in structuring diverse representations}{figure.caption.13}{}}
\newlabel{sub@fig:APP_C3_MCS}{{a}{14}{\textbf {Mean Cosine Similarity Across Depth With and Without Positional Embeddings} Removing positional embeddings from a trained Intact model increases similarity among token representations in early layers, reflecting a collapse of representational diversity. Intact models maintain lower early-layer similarity, highlighting the role of positional embeddings in structuring diverse representations}{figure.caption.13}{}}
\newlabel{fig:APP_C3_ERM}{{11b}{14}{\textbf {Effective Rank Across Depth With and Without Positional Embeddings.} Post-hoc ablation of positional embeddings substantially reduces effective rank, particularly in early layers, confirming that positional information supports the spread and dimensionality of learned representations. Intact models maintain higher rank across layers}{figure.caption.13}{}}
\newlabel{sub@fig:APP_C3_ERM}{{b}{14}{\textbf {Effective Rank Across Depth With and Without Positional Embeddings.} Post-hoc ablation of positional embeddings substantially reduces effective rank, particularly in early layers, confirming that positional information supports the spread and dimensionality of learned representations. Intact models maintain higher rank across layers}{figure.caption.13}{}}
\newlabel{fig:APP_C3_SSDC}{{11c}{14}{\textbf {SSDC Across Depth With and Without Positional Embeddings} Removing positional embeddings lowers initial SSDC and delays the characteristic peak from layer 2 to around layer 4, which is also weaker in magnitude. This indicates that positional embeddings accelerate the formation of spatial structure in early layers and contribute to its overall magnitude}{figure.caption.13}{}}
\newlabel{sub@fig:APP_C3_SSDC}{{c}{14}{\textbf {SSDC Across Depth With and Without Positional Embeddings} Removing positional embeddings lowers initial SSDC and delays the characteristic peak from layer 2 to around layer 4, which is also weaker in magnitude. This indicates that positional embeddings accelerate the formation of spatial structure in early layers and contribute to its overall magnitude}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Effects of Post-hoc Positional Embedding Removal.} Removing positional embeddings from a trained Intact model causes both representational diversity and SSDC to collapse.}}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:APP_C3_main}{{11}{14}{\textbf {Effects of Post-hoc Positional Embedding Removal.} Removing positional embeddings from a trained Intact model causes both representational diversity and SSDC to collapse}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Model Performance Across Distributional Shifts}{15}{subsection.C.4}\protected@file@percent }
\newlabel{fig:acc_normal}{{12a}{15}{Original Data}{figure.caption.14}{}}
\newlabel{sub@fig:acc_normal}{{a}{15}{Original Data}{figure.caption.14}{}}
\newlabel{fig:acc_adain}{{12b}{15}{AdaIN Style Transfer}{figure.caption.14}{}}
\newlabel{sub@fig:acc_adain}{{b}{15}{AdaIN Style Transfer}{figure.caption.14}{}}
\newlabel{fig:acc_blur}{{12c}{15}{Gaussian Blur}{figure.caption.14}{}}
\newlabel{sub@fig:acc_blur}{{c}{15}{Gaussian Blur}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Absolute Model Accuracies Across Distributional Shifts.} Intact models maintain substantially higher performance across all shifts, whereas Ablated and RPT-RPI Intact models exhibit similar, lower accuracies. This trend underscores the functional importance of intact positional embeddings.}}{15}{figure.caption.14}\protected@file@percent }
\newlabel{fig:accuracy_all}{{12}{15}{\textbf {Absolute Model Accuracies Across Distributional Shifts.} Intact models maintain substantially higher performance across all shifts, whereas Ablated and RPT-RPI Intact models exhibit similar, lower accuracies. This trend underscores the functional importance of intact positional embeddings}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Sanity Checks}{15}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Patch Content Reliance in RPT RPI Intact Models}{15}{subsection.D.1}\protected@file@percent }
\newlabel{fig:APP_D1_ERM}{{13b}{16}{\textbf {Representational Diversity in RPT–RPI Intact Models.} Effective Rank across layers for Intact and RPT–RPI Intact models largely overlaps, with only a slight reduction in the first layer for RPT–RPI Intact models. This indicates that representational diversity is largely preserved under random patch permutations, despite the absence of stable spatial structure}{figure.caption.15}{}}
\newlabel{sub@fig:APP_D1_ERM}{{b}{16}{\textbf {Representational Diversity in RPT–RPI Intact Models.} Effective Rank across layers for Intact and RPT–RPI Intact models largely overlaps, with only a slight reduction in the first layer for RPT–RPI Intact models. This indicates that representational diversity is largely preserved under random patch permutations, despite the absence of stable spatial structure}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Spatial Structure and Representational Diversity in RPT–RPI Intact Models.} This figure compares spatial structure and representational diversity in RPT–RPI Intact models and their non-permuted inference counterpart (RPT–NPI Intact), where patch permutations are removed at inference. SSDC remains near zero under random patch permutations but recovers healthy early-layer growth when permutations are removed, indicating content-dependent spatial structure. Effective Rank closely matches that of fully Intact models across layers, with only a slight reduction in the first layer, confirming that representational diversity is largely preserved. Together, these results validate that RPT–RPI Intact models maintain diverse representations while expressing spatial structure primarily through patch-content alignment.}}{16}{figure.caption.15}\protected@file@percent }
\newlabel{fig:APP_D1_main}{{13}{16}{\textbf {Spatial Structure and Representational Diversity in RPT–RPI Intact Models.} This figure compares spatial structure and representational diversity in RPT–RPI Intact models and their non-permuted inference counterpart (RPT–NPI Intact), where patch permutations are removed at inference. SSDC remains near zero under random patch permutations but recovers healthy early-layer growth when permutations are removed, indicating content-dependent spatial structure. Effective Rank closely matches that of fully Intact models across layers, with only a slight reduction in the first layer, confirming that representational diversity is largely preserved. Together, these results validate that RPT–RPI Intact models maintain diverse representations while expressing spatial structure primarily through patch-content alignment}{figure.caption.15}{}}
\gdef \@abspage@last{16}
